================================================================================
HOW TO USE THE EVALUATION SYSTEM
================================================================================

This evaluation system compares RAG (Retrieval-Augmented Generation) with
Llama 3.2 3B baseline on fake news classification.

Professional evaluation with real metrics, graphs, and detailed reports.
All output in English, no emojis.


================================================================================
FILES IN THIS DIRECTORY
================================================================================

MAIN EVALUATION:
  full_data_evaluation.py        - Main script to compare RAG vs Llama
                                   Generates metrics, graphs, and reports

UTILITIES:
  simple_evaluation.py           - Quick system check
  test_rag_only.py              - Test RAG retrieval only
  create_test_index.py          - Create small test index
  download_llama_model.py       - Download Llama model
  start_llama_server.py         - Start Llama server (if needed)

DOCUMENTATION:
  HOW_TO_USE.txt (this file)    - Quick usage guide
  PROFESSOR_GUIDE.txt           - Complete detailed guide
  COMPLETE_SYSTEM_SUMMARY.txt   - Full system documentation


================================================================================
QUICK START
================================================================================

STEP 1: Check System Status
----------------------------
python evaluate/simple_evaluation.py

This will tell you what's ready and what's missing.


STEP 2: Build Index (if needed)
--------------------------------
If you don't have a store yet:

cd index
python build_index_v3.py --input /path/to/your/data.csv --output store


STEP 3: Run Full Evaluation
----------------------------
python evaluate/full_data_evaluation.py \
  --data /path/to/test_data.csv \
  --store /path/to/store \
  --max-articles 100

Arguments:
  --data          Path to CSV file with test articles
  --store         Path to vector store directory
  --max-articles  Optional: limit number of articles to test
  --output        Optional: output directory (default: evaluate/results)


================================================================================
WHAT YOU GET
================================================================================

After running full_data_evaluation.py:

1. Console Output:
   - Progress updates
   - Performance metrics (Accuracy, Precision, Recall, F1)
   - Comparison analysis

2. JSON Report:
   File: evaluate/results/evaluation_report.json
   Contains:
   - Detailed metrics for both methods
   - Per-article results
   - Comparison analysis

3. Visualization Graphs:
   File: evaluate/results/comparison_graphs.png
   Shows:
   - Accuracy metrics comparison
   - Processing time comparison
   - Confusion matrices for both methods


================================================================================
EXAMPLE USAGE
================================================================================

Example 1: Test with small dataset
-----------------------------------
python evaluate/full_data_evaluation.py \
  --data data/test_articles.csv \
  --store index/store \
  --max-articles 50


Example 2: Full evaluation
---------------------------
python evaluate/full_data_evaluation.py \
  --data /StudentData/preprocessed/test.csv \
  --store index/store


Example 3: Custom output directory
-----------------------------------
python evaluate/full_data_evaluation.py \
  --data data/test.csv \
  --store index/store \
  --output my_evaluation_results


================================================================================
CSV DATA FORMAT
================================================================================

Your CSV file must have these columns:
  - label:   "fake" or "reliable"
  - title:   Article title
  - content: Article text

Optional columns:
  - id:      Article identifier


================================================================================
EXPECTED RESULTS
================================================================================

Typical performance (depends on data quality):

LLAMA BASELINE:
  Accuracy:  60-75%
  Speed:     1-2 seconds per article
  Pros:      Simple, fast
  Cons:      No external knowledge

RAG PIPELINE:
  Accuracy:  70-90%
  Speed:     3-5 seconds per article
  Pros:      Evidence-based, better accuracy
  Cons:      Slower due to retrieval

IMPROVEMENT:
  RAG typically improves accuracy by 10-15%
  Trade-off: 2-3x slower but more accurate


================================================================================
TROUBLESHOOTING
================================================================================

ISSUE: "Llama model not found"
SOLUTION: Download model first:
  python evaluate/download_llama_model.py

ISSUE: "Store directory not found"
SOLUTION: Build index first:
  cd index
  python build_index_v3.py --input your_data.csv --output store

ISSUE: "CSV missing required columns"
SOLUTION: Ensure CSV has columns: label, title, content

ISSUE: "Out of memory"
SOLUTION: Use --max-articles to limit dataset size:
  python evaluate/full_data_evaluation.py --data data.csv --store store --max-articles 100


================================================================================
BATCH PROCESSING (FOR LARGE DATASETS)
================================================================================

If you have a large dataset (e.g., 5000 articles), process in batches:

1. Split your data into smaller files:
   batch_001.csv (500 articles)
   batch_002.csv (500 articles)
   ...
   batch_010.csv (500 articles)

2. Run evaluation on each batch:
   for i in {1..10}; do
     python evaluate/full_data_evaluation.py \
       --data data/batch_$(printf "%03d" $i).csv \
       --store index/store \
       --output results/batch_$(printf "%03d" $i)
   done

3. Combine results manually or with custom script


================================================================================
TECHNICAL DETAILS
================================================================================

LLM Model:
  Name:     Llama 3.2 3B Instruct
  Size:     1.88 GB (4-bit quantized)
  Backend:  llama-cpp-python
  Hardware: CPU (no GPU required)

RAG System:
  Retrieval:    FAISS + BM25 hybrid
  Summarization: Contrastive summaries (fake vs reliable)
  Classification: Evidence-based decision

Metrics:
  Accuracy:  (TP + TN) / Total
  Precision: TP / (TP + FP)
  Recall:    TP / (TP + FN)
  F1 Score:  2 * (Precision * Recall) / (Precision + Recall)


================================================================================
FOR MORE INFORMATION
================================================================================

See PROFESSOR_GUIDE.txt for complete documentation including:
  - Detailed setup instructions
  - System architecture explanation
  - Expected results analysis
  - Troubleshooting guide


================================================================================
END
================================================================================
