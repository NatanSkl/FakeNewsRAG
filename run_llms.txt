Llama:
python -m llama_cpp.server \
  --model ./models/llama32-3b/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  --n_gpu_layers -1 --n_ctx 2048 --n_batch 256 --offload_kqv true \
  --host 127.0.0.1 --port 8010

Mistral:
python -m llama_cpp.server \
  --model ./models/mistral-7b-instruct/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --n_gpu_layers -1 --n_ctx 2048 --n_batch 256 --offload_kqv true \
  --host 127.0.0.1 --port 8011