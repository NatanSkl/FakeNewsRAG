 export CUDACXX="$CONDA_PREFIX/bin/nvcc"
 export CUDA_HOME="$CONDA_PREFIX"
 export CUDA_PATH="$CONDA_PREFIX"
 export CUDA_TOOLKIT_ROOT_DIR="$CONDA_PREFIX"
 export PATH="$CONDA_PREFIX/bin:$PATH"
 export LD_LIBRARY_PATH="$CONDA_PREFIX/lib:$LD_LIBRARY_PATH"

Llama:
python -m llama_cpp.server \
  --model ./models/llama32-3b/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  --n_gpu_layers -1 --n_ctx 4096 --n_batch 256 --offload_kqv true \
  --host 127.0.0.1 --port 8010

Mistral:
python -m llama_cpp.server \
  --model ./models/mistral-7b-instruct/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --n_gpu_layers -1 --n_ctx 2048 --n_batch 256 --offload_kqv true \
  --host 127.0.0.1 --port 8011

Gemma:
python -m llama_cpp.server \
  --model /home/student/models/gemma-3-1b-it-Q4_K_M.gguf \
  --n_gpu_layers -1 \
  --n_ctx 4096 \
  --n_batch 192 \
  --offload_kqv true \
  --host 127.0.0.1 --port 8012




curl http://127.0.0.1:8010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "local",
        "messages": [
          {"role":"system","content":"You are helpful."},
          {"role":"user","content":"Tell me a joke about bananas."}
        ],
        "max_tokens": 64,
        "temperature": 0.2
      }'
